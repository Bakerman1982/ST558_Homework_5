[
  {
    "objectID": "Homework05.html",
    "href": "Homework05.html",
    "title": "ST558 - Homework #5",
    "section": "",
    "text": "library(tidyverse)\nlibrary(caret)\nlibrary(class)\nlibrary(tree)\nlibrary(rpart)\nlibrary(gbm)\nlibrary(randomForest)"
  },
  {
    "objectID": "Homework05.html#task-1-conceptual-questions",
    "href": "Homework05.html#task-1-conceptual-questions",
    "title": "ST558 - Homework #5",
    "section": "Task 1: Conceptual Questions",
    "text": "Task 1: Conceptual Questions\nOn the exam, you’ll be asked to explain some topics. How about some practice?! Create a markdown list with the following questions:\n1. What is the purpose of using cross-validation when fitting a random forest model?\n\nThe purpose of cross-validating a random forest model is to evaluate its performance. We would be interested in analyzing overfittedness and precision.\n2. Describe the bagged tree algorithm.\n\nThe bagged-tree algorithm works by taking repeated samples of a data set. A parameter is taken from each sample. A distribution is derived from that collection of sampled parameters and the theory is that we can obtain an estimator from that distribution of estimators that will explain the data.\n3. What is meant by a general linear model?\n\nA general linear model describes a rigid transactional relationship between a dependent and independent variable that can be expressed linearly; one observation to one outcome at various points along the data.\n4. When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nThe interaction term accounts for a relationship between two independent variables. Adding an interaction term to a model can either be informative or not helpful at all. It can be helpful in terms of explaining how two variables have effects on each other and not simply a relationship that is additive (i.e. not simply adding \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2\\); rather \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2\\)). There exists scenarios where this can be harmful to modeling. If we were to add an interaction term that had no significance, likely it would not contribute any new information to explaining the variables in the model and at the cost of degrees of freedom which we require for statistical inference.\n5. Why do we split our data into a training and test set?\n\nA training set is the data we use to train a model while the test set is used to evaluate the model. The test set uses unseen data which acts as a unbiased-filter and we use that to estimate how well the model in the training set is performing."
  },
  {
    "objectID": "Homework05.html#task-2-fitting-models",
    "href": "Homework05.html#task-2-fitting-models",
    "title": "ST558 - Homework #5",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\nWe’ll use the data set called heart.csv available here. This data set gives information about whether or not someone has heart disease (HeartDisease = 1 or = 0) along with different measurements about that person’s health. The data comes from here if you’d like to read a bit more about it.\n\n\nQuick EDA/Data Preparation\n1. Quickly understand your data. Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\n#Import Data \nheart_data_unedited &lt;- read.csv(\"heart.csv\")\n\n#Convert data to Tibble\nheart_data &lt;- as_tibble(heart_data_unedited)\n\n#Check Data for missing values\ncolSums(is.na(heart_data))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n#Summarize data wrt HeartDisease; nested many of the numerical data points into the summarise function and called on min, max, mean, and sd.  I performed a round on any value that had a 100th place decimal or smaller and called on glimpse to display all values.  \nheart_data_summary &lt;- heart_data |&gt;\n  group_by(HeartDisease) |&gt;\n  summarise(\n    HeartDiseaseCount = n(), \n    min_age = min(Age, na.rm = TRUE),\n    average_age = round(mean(Age, na.rm = TRUE),1),\n    max_age = max(Age, na.rm = TRUE),\n    sd_age = round(sd(Age, na.rm = TRUE),1),\n    min_RestingBP = min(RestingBP, na.rm = TRUE),\n    averRestingBP_RestingBP = round(mean(RestingBP, na.rm = TRUE),1),\n    max_RestingBP = max(RestingBP, na.rm = TRUE),\n    sd_RestingBP = round(sd(RestingBP, na.rm = TRUE),1),\n    min_Cholesterol = min(Cholesterol, na.rm = TRUE),\n    averCholesterol_Cholesterol = round(mean(Cholesterol, na.rm = TRUE),1),\n    max_Cholesterol = max(Cholesterol, na.rm = TRUE),\n    sd_Cholesterol = round(sd(Cholesterol, na.rm = TRUE),1))\nglimpse(heart_data_summary)\n\nRows: 2\nColumns: 14\n$ HeartDisease                &lt;int&gt; 0, 1\n$ HeartDiseaseCount           &lt;int&gt; 410, 508\n$ min_age                     &lt;int&gt; 28, 31\n$ average_age                 &lt;dbl&gt; 50.6, 55.9\n$ max_age                     &lt;int&gt; 76, 77\n$ sd_age                      &lt;dbl&gt; 9.4, 8.7\n$ min_RestingBP               &lt;int&gt; 80, 0\n$ averRestingBP_RestingBP     &lt;dbl&gt; 130.2, 134.2\n$ max_RestingBP               &lt;int&gt; 190, 200\n$ sd_RestingBP                &lt;dbl&gt; 16.5, 19.8\n$ min_Cholesterol             &lt;int&gt; 0, 0\n$ averCholesterol_Cholesterol &lt;dbl&gt; 227.1, 175.9\n$ max_Cholesterol             &lt;int&gt; 564, 603\n$ sd_Cholesterol              &lt;dbl&gt; 74.6, 126.4\n\n\n2. Create a new variable that is a factor version of the HeartDisease variable (if needed, this depends on how you read in your data). Remove the ST_Slope variable and the original HeartDisease variable (if applicable).\n\n\n#Got rid of Cholesterol values that were not possible (tipped off from the message boards), set HeartDisease as a factor and replaced the existing column with the factorized one, and finally removed ST_Slope from the tibble. \nheart_data &lt;- heart_data |&gt;\n  filter(Cholesterol &gt; 0) |&gt;\n  mutate(HeartDisease = as.factor(HeartDisease)) %&gt;%\n  select(-ST_Slope)\nprint(heart_data)\n\n# A tibble: 746 × 11\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;             &lt;int&gt;       &lt;int&gt;     &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n 1    40 M     ATA                 140         289         0 Normal       172\n 2    49 F     NAP                 160         180         0 Normal       156\n 3    37 M     ATA                 130         283         0 ST            98\n 4    48 F     ASY                 138         214         0 Normal       108\n 5    54 M     NAP                 150         195         0 Normal       122\n 6    39 M     NAP                 120         339         0 Normal       170\n 7    45 F     ATA                 130         237         0 Normal       170\n 8    54 M     ATA                 110         208         0 Normal       142\n 9    37 M     ASY                 140         207         0 Normal       130\n10    48 F     ATA                 120         284         0 Normal       120\n# ℹ 736 more rows\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\n3. We’ll be doing a kNN model below to predict whether or not someone has heart disease. To use kNN we generally want to have all numeric predictors (although we could try to create our own loss function as an alternative). In this case we have some categorical predictors still in our data set: Sex, ExerciseAngina ChestPainType, and RestingECG.\nCreate dummy columns corresponding to the values of these three variables for use in our kNN fit. The caret vignette has a function to help us out here. You should use dummyVars() and predict() to create new columns. Then add these columns to our data frame.\n\n\n#Created a dummy object `dmy` with the columns that I want to \"dummify\".\ndmy &lt;- dummyVars(\"~ Sex + ChestPainType + RestingECG + ExerciseAngina\", heart_data)\n#Create a predict_heartdata object to hold the predicted dmy values for adding to the main tibble. \npred_hd &lt;- predict(dmy, heart_data)\n\n#Adding the predicted values to the table while removed their non-predictive variants. \nheart_data &lt;- bind_cols(\n  heart_data %&gt;%\n    select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG), pred_hd)\nprint(heart_data)\n\n# A tibble: 746 × 18\n     Age RestingBP Cholesterol FastingBS MaxHR Oldpeak HeartDisease  SexF  SexM\n   &lt;int&gt;     &lt;int&gt;       &lt;int&gt;     &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1    40       140         289         0   172     0   0                0     1\n 2    49       160         180         0   156     1   1                1     0\n 3    37       130         283         0    98     0   0                0     1\n 4    48       138         214         0   108     1.5 1                1     0\n 5    54       150         195         0   122     0   0                0     1\n 6    39       120         339         0   170     0   0                0     1\n 7    45       130         237         0   170     0   0                1     0\n 8    54       110         208         0   142     0   0                0     1\n 9    37       140         207         0   130     1.5 1                0     1\n10    48       120         284         0   120     0   0                1     0\n# ℹ 736 more rows\n# ℹ 9 more variables: ChestPainTypeASY &lt;dbl&gt;, ChestPainTypeATA &lt;dbl&gt;,\n#   ChestPainTypeNAP &lt;dbl&gt;, ChestPainTypeTA &lt;dbl&gt;, RestingECGLVH &lt;dbl&gt;,\n#   RestingECGNormal &lt;dbl&gt;, RestingECGST &lt;dbl&gt;, ExerciseAnginaN &lt;dbl&gt;,\n#   ExerciseAnginaY &lt;dbl&gt;"
  },
  {
    "objectID": "Homework05.html#split-your-data",
    "href": "Homework05.html#split-your-data",
    "title": "ST558 - Homework #5",
    "section": "Split your Data",
    "text": "Split your Data\nSplit your data into a training and test set. (Ideally you’d do this prior to the EDA so that info from the EDA doesn’t bias what you do modeling-wise, but that isn’t usually done.)\n\n\n#Reproducibility.\nset.seed(1234)\n#Split on HeartDisease, by 70:30 ratio. Object name describes which split is which. \nheart_split &lt;- createDataPartition(heart_data$HeartDisease, p = 0.7, list = FALSE)\nheart_train &lt;- heart_data[heart_split, ]\nheart_test &lt;- heart_data[-heart_split, ]"
  },
  {
    "objectID": "Homework05.html#knn",
    "href": "Homework05.html#knn",
    "title": "ST558 - Homework #5",
    "section": "kNN",
    "text": "kNN\nNext, we’ll fit a kNN model. The article here gives a great example of selecting the number of neighbors to use with the caret package.\n\nYou don’t have to use all the variables from your dataset when fitting the model. However, you should only use numeric variables.\n\nThey use repeated 10 fold cross-validation. Although computationally intensive, doing repeated CV helps to give a more stable prediction of CV error. This is similar to how a mean is less variable than a single value. Since there is some inherent randomness in doing a CV computation, we can get an overall more stable result by averaging a few runs of the CV algorithm!\n\nTrain the kNN model. Use repeated 10 fold cross-validation, with the number of repeats being 3. You should also preprocess the data by centering and scaling. When fitting the model, set the tuneGrid so that you are considering values of k of 1, 2, 3, . . . , 40. (Note: From the help for the train() function it says: tuneGrid A data frame with possible tuning values. The columns are named the same as the tuning parameters. The name of the tuning parameter here is k.)\n\n\n#built-in caret function to control the training computations.  Ten-Fold cross validation is used with three repeats. \ntrainCTRL &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3);\n\n#hyperparameter Tuning \ntuneGrid &lt;- expand.grid(k = 1:40)\n\n#The substance of the kNN model with reproducibility and center/scaling. \nset.seed(1234)\nheartData_knn_model &lt;- train(\n  HeartDisease ~ Age + Cholesterol,\n  heart_train, \n  method = \"knn\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\nprint(heartData_knn_model)\n\nk-Nearest Neighbors \n\n523 samples\n  2 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (2), scaled (2) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 470, 471, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa     \n   1  0.5329221  0.06324044\n   2  0.5597242  0.11823594\n   3  0.5629415  0.12363517\n   4  0.5629052  0.12469367\n   5  0.5703677  0.14052297\n   6  0.5984156  0.19676275\n   7  0.5985123  0.19762148\n   8  0.6016570  0.20418402\n   9  0.6112361  0.22387650\n  10  0.6156991  0.23136246\n  11  0.6126149  0.22547391\n  12  0.6092525  0.21750064\n  13  0.6182269  0.23588711\n  14  0.6118529  0.22294697\n  15  0.6220972  0.24383373\n  16  0.6234277  0.24708692\n  17  0.6258829  0.25103848\n  18  0.6265965  0.25261461\n  19  0.6208394  0.24090319\n  20  0.6195210  0.23849886\n  21  0.6220851  0.24399254\n  22  0.6264998  0.25239692\n  23  0.6291001  0.25777563\n  24  0.6309869  0.26086611\n  25  0.6310353  0.26097367\n  26  0.6354378  0.26997172\n  27  0.6335631  0.26575918\n  28  0.6303701  0.25972044\n  29  0.6328979  0.26483949\n  30  0.6399371  0.27982576\n  31  0.6450774  0.28968578\n  32  0.6443638  0.28826772\n  33  0.6488268  0.29755142\n  34  0.6513788  0.30237272\n  35  0.6571118  0.31359208\n  36  0.6514272  0.30233526\n  37  0.6495041  0.29855678\n  38  0.6436986  0.28713567\n  39  0.6552129  0.31005380\n  40  0.6481979  0.29595141\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 35.\n\n\nLastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\n\nheart_test_predictions &lt;- predict(heartData_knn_model, newdata = heart_test)\nConfMtrx &lt;- confusionMatrix(heart_test_predictions, heart_test$HeartDisease)\nConfMtrx\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 73 42\n         1 44 64\n                                         \n               Accuracy : 0.6143         \n                 95% CI : (0.547, 0.6786)\n    No Information Rate : 0.5247         \n    P-Value [Acc &gt; NIR] : 0.004305       \n                                         \n                  Kappa : 0.2275         \n                                         \n Mcnemar's Test P-Value : 0.914128       \n                                         \n            Sensitivity : 0.6239         \n            Specificity : 0.6038         \n         Pos Pred Value : 0.6348         \n         Neg Pred Value : 0.5926         \n             Prevalence : 0.5247         \n         Detection Rate : 0.3274         \n   Detection Prevalence : 0.5157         \n      Balanced Accuracy : 0.6139         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nMainly interested in the accuracy = 0.6143. That is from the reference matrix (73 + 64)/223. The diagonal divided by the sum of the entire matrix."
  },
  {
    "objectID": "Homework05.html#logistic-regression",
    "href": "Homework05.html#logistic-regression",
    "title": "ST558 - Homework #5",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nUsing your EDA, posit three different logistic regression models. Note: You don’t have to use the dummy columns you made here as the glm() function (and the caret implementation of it) can handle factor/character variables as predictors.\n\n\n#Model with the non-binary variables\nmodel1 &lt;- HeartDisease ~ Age + SexF + SexM + Cholesterol + RestingBP\n\n#Model with some non-binary variables thrown together\nmodel2 &lt;- HeartDisease ~ Age + SexF + SexM + Cholesterol + ExerciseAnginaN + ExerciseAnginaY + FastingBS\n\n#Model using all variables\nmodel3 &lt;- HeartDisease ~ .\n\nFit those models on the training set, using repeated CV as done above. You can preprocess the data or not, up to you.\n\n\nset.seed(1234)\nmodel1_train &lt;- train(\n  model1,\n  heart_train, \n  method = \"knn\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\n\nset.seed(1234)\nmodel2_train &lt;- train(\n  model2,\n  heart_train, \n  method = \"knn\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\n\nset.seed(1234)\nmodel3_train &lt;- train(\n  model3,\n  heart_train, \n  method = \"knn\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\n\n\n# Extract resamples into a list\nresults &lt;- resamples(list(model1 = model1_train, model2 = model2_train, model3 = model3_train))\nprint(results)\n\n\nCall:\nresamples.default(x = list(model1 = model1_train, model2 = model2_train,\n model3 = model3_train))\n\nModels: model1, model2, model3 \nNumber of resamples: 30 \nPerformance metrics: Accuracy, Kappa \nTime estimates for: everything, final model fit \n\n\nIdentify your best model and provide a basic summary of it.\n\n\n# Summarize the results\nresults_summary &lt;- summary(results)\nprint(results_summary)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: model1, model2, model3 \nNumber of resamples: 30 \n\nAccuracy \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.5769231 0.6346154 0.6826923 0.6947388 0.7500000 0.8461538    0\nmodel2 0.6153846 0.7393868 0.7692308 0.7667392 0.7884615 0.8846154    0\nmodel3 0.6346154 0.7894594 0.8269231 0.8215893 0.8672896 0.9230769    0\n\nKappa \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.1398496 0.2681433 0.3667441 0.3881459 0.5000000 0.6936672    0\nmodel2 0.2133132 0.4750450 0.5350224 0.5287619 0.5753519 0.7675112    0\nmodel3 0.2648810 0.5774200 0.6523024 0.6421745 0.7337140 0.8454681    0\n\n\nLastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\n\n# Extract accuracies\nmodel_accuracies &lt;- results_summary$statistics$Accuracy[, \"Mean\"]\n\n# Identify the best model\nbest_model_name &lt;- names(which.max(model_accuracies))\nprint(paste(\"Best model:\", best_model_name))\n\n[1] \"Best model: model3\"\n\n# Retrieve the best model\nbest_model_train &lt;- get(paste0(best_model_name, \"_train\"))\nprint(best_model_train)\n\nk-Nearest Neighbors \n\n523 samples\n 17 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 470, 471, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7756531  0.5494992\n   2  0.7604257  0.5192619\n   3  0.8036647  0.6057501\n   4  0.8125907  0.6238228\n   5  0.8215893  0.6421745\n   6  0.8088534  0.6168747\n   7  0.8062772  0.6116055\n   8  0.8095307  0.6180411\n   9  0.8024673  0.6038485\n  10  0.8043541  0.6073807\n  11  0.8043662  0.6077426\n  12  0.8075593  0.6140501\n  13  0.8018626  0.6026277\n  14  0.7980406  0.5950520\n  15  0.7999516  0.5989053\n  16  0.7960813  0.5910060\n  17  0.8030842  0.6050177\n  18  0.8036768  0.6063192\n  19  0.7980044  0.5946424\n  20  0.8005806  0.5997167\n  21  0.7967223  0.5919702\n  22  0.8024673  0.6035206\n  23  0.7947992  0.5880391\n  24  0.7954765  0.5895748\n  25  0.7935414  0.5855550\n  26  0.8005806  0.5996907\n  27  0.7954765  0.5895183\n  28  0.7987059  0.5960288\n  29  0.7986575  0.5960733\n  30  0.8018868  0.6028120\n  31  0.7986817  0.5963794\n  32  0.7986696  0.5962821\n  33  0.7955007  0.5900747\n  34  0.7910377  0.5813189\n  35  0.7923198  0.5838908\n  36  0.7974117  0.5940047\n  37  0.7980285  0.5953016\n  38  0.8031688  0.6055957\n  39  0.7993469  0.5978469\n  40  0.8000121  0.5992568\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 5.\n\n# Evaluate the best model on the test set\npredictions &lt;- predict(best_model_train, newdata = heart_test)\nconf_matrix &lt;- confusionMatrix(predictions, heart_test$HeartDisease)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 98 27\n         1 19 79\n                                          \n               Accuracy : 0.7937          \n                 95% CI : (0.7346, 0.8448)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.585           \n                                          \n Mcnemar's Test P-Value : 0.302           \n                                          \n            Sensitivity : 0.8376          \n            Specificity : 0.7453          \n         Pos Pred Value : 0.7840          \n         Neg Pred Value : 0.8061          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4395          \n   Detection Prevalence : 0.5605          \n      Balanced Accuracy : 0.7914          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework05.html#tree-models",
    "href": "Homework05.html#tree-models",
    "title": "ST558 - Homework #5",
    "section": "Tree Models",
    "text": "Tree Models\nIn this section we’ll fit a few different tree based models in a similar way as above!\n\nChoose your own variables of interest (as with logistic regression, this models can accept factor/character variables as predictors). Use repeated 10 fold CV to select a best:\n\n#I was not sure if we were supposed to choose the best model from earlier, one of the other models from earlier or select a new sets of independent variables hypothesized to explain most of what makes up this model; so I chose to assume the . \nmodel4 &lt;- HeartDisease ~ Age + SexF + SexM + Cholesterol + RestingBP\nmodel5 &lt;- HeartDisease ~ FastingBS + ChestPainTypeATA + ChestPainTypeNAP + ChestPainTypeASY + ChestPainTypeTA\nmodel6 &lt;- HeartDisease ~ RestingECGNormal + RestingECGST + RestingECGLVH + Cholesterol\n\n#built-in caret function to control the training computations.  Ten-Fold cross validation is used with three repeats. \ntrainCTRL &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3);\n\n#hyperparameter Tuning \ntuneGrid &lt;- expand.grid(k = 1:40)\n\n#Using the trainCTRL and tuneGrid objects to replicate the same environment to training model4 as I did model1-model3.\nset.seed(1234)\nmodel4_train &lt;- train(\n  model4,\n  data = heart_train, \n  method = \"knn\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\n\nset.seed(1234)\nmodel5_train &lt;- train(\n  model5,\n  data = heart_train, \n  method = \"knn\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\n\nset.seed(1234)\nmodel6_train &lt;- train(\n  model6,\n  data = heart_train, \n  method = \"knn\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Extract samples from models 4,5 and 6 and add them to the existing list of models sampled. \nresults &lt;- resamples(list(model4 = model4_train, model5 = model5_train, model6 = model6_train))\n\n# Summarize the results\nresults_summary &lt;- summary(results)\n\n# Extract accuracies\nmodel_accuracies &lt;- results_summary$statistics$Accuracy[, \"Mean\"]\n\n# Identify the best model\nbest_model_name &lt;- names(which.max(model_accuracies))\nprint(paste(\"Best model:\", best_model_name))\n\n[1] \"Best model: model5\"\n\n# Retrieve the best model\nbest_model_train &lt;- get(paste0(best_model_name, \"_train\"))\nprint(best_model_train)\n\nk-Nearest Neighbors \n\n523 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 470, 471, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7796323  0.5595132\n   2  0.7796323  0.5595132\n   3  0.7802733  0.5608065\n   4  0.7796323  0.5595132\n   5  0.7789913  0.5582161\n   6  0.7764514  0.5526786\n   7  0.7764514  0.5526786\n   8  0.7764514  0.5526786\n   9  0.7764514  0.5526786\n  10  0.7764514  0.5526786\n  11  0.7764514  0.5526786\n  12  0.7764514  0.5526786\n  13  0.7764514  0.5526786\n  14  0.7764514  0.5526786\n  15  0.7764514  0.5526786\n  16  0.7764514  0.5526786\n  17  0.7758104  0.5514198\n  18  0.7758104  0.5514889\n  19  0.7751693  0.5502331\n  20  0.7745283  0.5489119\n  21  0.7745283  0.5489119\n  22  0.7751693  0.5501640\n  23  0.7738873  0.5481460\n  24  0.7726294  0.5461804\n  25  0.7732946  0.5480020\n  26  0.7707547  0.5434724\n  27  0.7707426  0.5434988\n  28  0.7707426  0.5434988\n  29  0.7707426  0.5434988\n  30  0.7707426  0.5434988\n  31  0.7707426  0.5434988\n  32  0.7707426  0.5434988\n  33  0.7707426  0.5434988\n  34  0.7707426  0.5434988\n  35  0.7707426  0.5434988\n  36  0.7707426  0.5434988\n  37  0.7707426  0.5434988\n  38  0.7707426  0.5434988\n  39  0.7707426  0.5434988\n  40  0.7707426  0.5434988\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 3.\n\n# Evaluate the best model on the test set\npredictions &lt;- predict(best_model_train, newdata = heart_test)\nconf_matrix &lt;- confusionMatrix(predictions, heart_test$HeartDisease)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 82 28\n         1 35 78\n                                          \n               Accuracy : 0.7175          \n                 95% CI : (0.6535, 0.7756)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 3.159e-09       \n                                          \n                  Kappa : 0.4354          \n                                          \n Mcnemar's Test P-Value : 0.4497          \n                                          \n            Sensitivity : 0.7009          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7455          \n         Neg Pred Value : 0.6903          \n             Prevalence : 0.5247          \n         Detection Rate : 0.3677          \n   Detection Prevalence : 0.4933          \n      Balanced Accuracy : 0.7184          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nBased on the results we are still going to move forward with Model #5 as the best model to use for the subsequent tree modeling.\n• classification tree model (use method = rpart: tuning parameter is cp, use values 0, 0.001, 0.002, . . . ,0.1)\n\n\n#Using the example in the slides as a resource, I used the original heart_data to fitTree.  Was not sure we were supposed to use the best model from earlier.  Noticed that the slides used the original dataset and not the modeling sets.  Apologies, this section was a little more confusing for me than earlier assignments. \n\n#This handles the intructions of method and tuning parameter to be used in the training\ntuneGridClass &lt;- expand.grid(cp = seq(0, 0.1, by = 0.001))\n\n#training the model\nset.seed(1234)\nclassification_tree &lt;- train(\n  model5,\n  data = heart_train,\n  method = \"rpart\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGridClass,\n  preProcess = c(\"center\", \"scale\")\n)\nprint(classification_tree)\n\nCART \n\n523 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 470, 471, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7764514  0.5526786\n  0.001  0.7764514  0.5526786\n  0.002  0.7764514  0.5526786\n  0.003  0.7764514  0.5526786\n  0.004  0.7764514  0.5526786\n  0.005  0.7764514  0.5526786\n  0.006  0.7764514  0.5526786\n  0.007  0.7764514  0.5526786\n  0.008  0.7764514  0.5526786\n  0.009  0.7764514  0.5526786\n  0.010  0.7764514  0.5526786\n  0.011  0.7764514  0.5526786\n  0.012  0.7764514  0.5526786\n  0.013  0.7764514  0.5526786\n  0.014  0.7764514  0.5526786\n  0.015  0.7764514  0.5526786\n  0.016  0.7764514  0.5526786\n  0.017  0.7764514  0.5526786\n  0.018  0.7764514  0.5526786\n  0.019  0.7764514  0.5526786\n  0.020  0.7764514  0.5526786\n  0.021  0.7764514  0.5526786\n  0.022  0.7764514  0.5526786\n  0.023  0.7764514  0.5526786\n  0.024  0.7764514  0.5526786\n  0.025  0.7764514  0.5526786\n  0.026  0.7764514  0.5526786\n  0.027  0.7764514  0.5526786\n  0.028  0.7764514  0.5526786\n  0.029  0.7764514  0.5526786\n  0.030  0.7764514  0.5526786\n  0.031  0.7764514  0.5526786\n  0.032  0.7764514  0.5526786\n  0.033  0.7764514  0.5526786\n  0.034  0.7764514  0.5526786\n  0.035  0.7764514  0.5526786\n  0.036  0.7764514  0.5526786\n  0.037  0.7764514  0.5526786\n  0.038  0.7764514  0.5526786\n  0.039  0.7764514  0.5526786\n  0.040  0.7764514  0.5526786\n  0.041  0.7764514  0.5526786\n  0.042  0.7764514  0.5526786\n  0.043  0.7764514  0.5526786\n  0.044  0.7764514  0.5526786\n  0.045  0.7764514  0.5526786\n  0.046  0.7764514  0.5526786\n  0.047  0.7764514  0.5526786\n  0.048  0.7764514  0.5526786\n  0.049  0.7764514  0.5526786\n  0.050  0.7764514  0.5526786\n  0.051  0.7764514  0.5526786\n  0.052  0.7764514  0.5526786\n  0.053  0.7764514  0.5526786\n  0.054  0.7764514  0.5526786\n  0.055  0.7764514  0.5526786\n  0.056  0.7764514  0.5526786\n  0.057  0.7764514  0.5526786\n  0.058  0.7764514  0.5526786\n  0.059  0.7764514  0.5526786\n  0.060  0.7764514  0.5526786\n  0.061  0.7764514  0.5526786\n  0.062  0.7764514  0.5526786\n  0.063  0.7764514  0.5526786\n  0.064  0.7764514  0.5526786\n  0.065  0.7764514  0.5526786\n  0.066  0.7764514  0.5526786\n  0.067  0.7764514  0.5526786\n  0.068  0.7764514  0.5526786\n  0.069  0.7764514  0.5526786\n  0.070  0.7764514  0.5526786\n  0.071  0.7764514  0.5526786\n  0.072  0.7764514  0.5526786\n  0.073  0.7764514  0.5526786\n  0.074  0.7764514  0.5526786\n  0.075  0.7764514  0.5526786\n  0.076  0.7764514  0.5526786\n  0.077  0.7764514  0.5526786\n  0.078  0.7764514  0.5526786\n  0.079  0.7764514  0.5526786\n  0.080  0.7764514  0.5526786\n  0.081  0.7764514  0.5526786\n  0.082  0.7764514  0.5526786\n  0.083  0.7764514  0.5526786\n  0.084  0.7764514  0.5526786\n  0.085  0.7764514  0.5526786\n  0.086  0.7764514  0.5526786\n  0.087  0.7764514  0.5526786\n  0.088  0.7764514  0.5526786\n  0.089  0.7764514  0.5526786\n  0.090  0.7764514  0.5526786\n  0.091  0.7764514  0.5526786\n  0.092  0.7764514  0.5526786\n  0.093  0.7764514  0.5526786\n  0.094  0.7764514  0.5526786\n  0.095  0.7764514  0.5526786\n  0.096  0.7764514  0.5526786\n  0.097  0.7764514  0.5526786\n  0.098  0.7764514  0.5526786\n  0.099  0.7764514  0.5526786\n  0.100  0.7764514  0.5526786\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.1.\n\n\n• a random forest (use method = rf: tuning parameter is mtry, use values of 1, 2, . . . , # of predictors (bagging is a special case here!)\n\n\n# Set up empty var for tuning.\nnum_predictors &lt;- length(all.vars(model5)) - 1  \n\n# New Tuning for this tree fit\ntuneGridRando &lt;- expand.grid(mtry = 1:num_predictors)\n\nset.seed(1234)\nrandom_forest &lt;- train(\n  model5,\n  data = heart_train,\n  method = \"rf\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGridRando,\n  preProcess = c(\"center\", \"scale\")\n)\nprint(random_forest)\n\nRandom Forest \n\n523 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 470, 471, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  1     0.7745283  0.5490890\n  2     0.7732583  0.5464946\n  3     0.7777334  0.5556331\n  4     0.7789913  0.5582161\n  5     0.7789913  0.5582161\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 4.\n\n\n• a boosted tree (use method = gbm: tuning parameters are n.trees, interaction.depth, shrinkage, and n.minobsinnode, use all combinations of n.trees of 25, 50, 100, and 200, interaction.depth of 1, 2, 3, shrinkage = 0.1, and nminobsinnode = 10; Hint: use expand.grid() to create your data frame for tuneGrid and verbose = FALSE limits the output produced.\n\n\n# Define the train control for cross-validation\ntrainCTRL &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n# Define the tuning grid for the boosted tree\ntuneGridBoosted &lt;- expand.grid(\n  n.trees = c(25, 50, 100, 200),\n  interaction.depth = c(1, 2, 3),\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n\n# Train the boosted tree model using caret\nset.seed(1234)\nboosted_tree &lt;- train(\n  model5,\n  data = heart_train,\n  method = \"gbm\",\n  trControl = trainCTRL,\n  tuneGrid = tuneGridBoosted,\n  preProcess = c(\"center\", \"scale\"),\n  verbose = FALSE\n)\n# Print the model summary to check the best parameters\nprint(boosted_tree)\n\nStochastic Gradient Boosting \n\n523 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 470, 471, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.7764514  0.5526786\n  1                   50      0.7789913  0.5581773\n  1                  100      0.7764393  0.5533593\n  1                  200      0.7796444  0.5595999\n  2                   25      0.7790034  0.5582189\n  2                   50      0.7802733  0.5608065\n  2                  100      0.7802733  0.5608065\n  2                  200      0.7802733  0.5608065\n  3                   25      0.7796323  0.5595093\n  3                   50      0.7789913  0.5582991\n  3                  100      0.7802733  0.5608065\n  3                  200      0.7802733  0.5608065\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 2, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\nLastly, check how well each of your chosen models do on the test set using the confusionMatrix() function\n\n\n# Test set predictions\npred_tree &lt;- predict(classification_tree, newdata = heart_test)\npred_rf &lt;- predict(random_forest, newdata = heart_test)\npred_boosted &lt;- predict(boosted_tree, newdata = heart_test)\n\n# Confusion matrices\nconf_matrix_tree &lt;- confusionMatrix(pred_tree, heart_test$HeartDisease); print(conf_matrix_tree)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 85 29\n         1 32 77\n                                          \n               Accuracy : 0.7265          \n                 95% CI : (0.6629, 0.7838)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 5.586e-10       \n                                          \n                  Kappa : 0.4523          \n                                          \n Mcnemar's Test P-Value : 0.7979          \n                                          \n            Sensitivity : 0.7265          \n            Specificity : 0.7264          \n         Pos Pred Value : 0.7456          \n         Neg Pred Value : 0.7064          \n             Prevalence : 0.5247          \n         Detection Rate : 0.3812          \n   Detection Prevalence : 0.5112          \n      Balanced Accuracy : 0.7265          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconf_matrix_rf &lt;- confusionMatrix(pred_rf, heart_test$HeartDisease); print(conf_matrix_rf)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 82 28\n         1 35 78\n                                          \n               Accuracy : 0.7175          \n                 95% CI : (0.6535, 0.7756)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 3.159e-09       \n                                          \n                  Kappa : 0.4354          \n                                          \n Mcnemar's Test P-Value : 0.4497          \n                                          \n            Sensitivity : 0.7009          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7455          \n         Neg Pred Value : 0.6903          \n             Prevalence : 0.5247          \n         Detection Rate : 0.3677          \n   Detection Prevalence : 0.4933          \n      Balanced Accuracy : 0.7184          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconf_matrix_boosted &lt;- confusionMatrix(pred_boosted, heart_test$HeartDisease); print(conf_matrix_boosted)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 82 28\n         1 35 78\n                                          \n               Accuracy : 0.7175          \n                 95% CI : (0.6535, 0.7756)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 3.159e-09       \n                                          \n                  Kappa : 0.4354          \n                                          \n Mcnemar's Test P-Value : 0.4497          \n                                          \n            Sensitivity : 0.7009          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7455          \n         Neg Pred Value : 0.6903          \n             Prevalence : 0.5247          \n         Detection Rate : 0.3677          \n   Detection Prevalence : 0.4933          \n      Balanced Accuracy : 0.7184          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework05.html#wrap-up",
    "href": "Homework05.html#wrap-up",
    "title": "ST558 - Homework #5",
    "section": "Wrap up",
    "text": "Wrap up\nWhich model overall did the best job (in terms of accuracy) on the test set?\n\n\n# Extract balanced accuracy from each confusion matrix\nbalanced_accuracy_tree &lt;- conf_matrix_tree$byClass[11]\nbalanced_accuracy_rf &lt;- conf_matrix_rf$byClass[11]\nbalanced_accuracy_boosted &lt;- conf_matrix_boosted$byClass[11]\n\n# Create a data frame to summarize the results\nbalanced_accuracy_summary &lt;- data.frame(\n  Model = c(\"Classification Tree\", \"Random Forest\", \"Boosted Tree\"),\n  Balanced_Accuracy = c(balanced_accuracy_tree, balanced_accuracy_rf, balanced_accuracy_boosted)\n)\n\n# Print the summary data frame\nprint(balanced_accuracy_summary)\n\n                Model Balanced_Accuracy\n1 Classification Tree         0.7264554\n2       Random Forest         0.7183519\n3        Boosted Tree         0.7183519\n\n\nOn accuracy alone the classification tree model for model5 outperformed the boosted tree and the random forest tree."
  }
]